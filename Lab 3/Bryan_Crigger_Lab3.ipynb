{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions\n",
        " * The research question is can we use algorithms and compute to identify clothing items? Specifically, can we determine which algorithm and compute methodology provides us the most efficient approach for classifying simple fashion images?\n",
        " * Using the base samples available from Zalando Research:\n",
        "  * https://github.com/zalandoresearch/fashion-mnist\n",
        "  * Review the data -- clean as appropriate\n",
        "  * Provide an initial data analysis\n",
        " * Implement at least two approaches for classifying the digits -- examples below:\n",
        "  * Na√Øve bayes\n",
        "  * Neural Networks\n",
        "  * Keras\n",
        "  * Azure ML\n",
        "  * IBM DSX\n",
        "  * Boosted trees\n",
        "  * Linear classification\n",
        "  * Your choice\n",
        "\n",
        "* Answer the following questions:\n",
        "  * What is the accuracy of each method?\n",
        "  * What are the trade-offs of each approach?\n",
        "  * What is the compute performance of each approach"
      ],
      "metadata": {
        "id": "wZQMEdpQyPI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Keras Model"
      ],
      "metadata": {
        "id": "_0eaJzvbycc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "wvQM-5GryTml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# -----------------------------------\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "qVwC0ByqyRLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "nAa31HZeyRI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q -U keras-tuner"
      ],
      "metadata": {
        "id": "7_nL-rFuyRGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading in the data, Splitting data into Training/Testing datasets, and reshapping the images"
      ],
      "metadata": {
        "id": "3GC89Dm0yeWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "LJZ-f28gyREM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images  = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "Y_grGHyEyRBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing some examples of the clothing data"
      ],
      "metadata": {
        "id": "RxvG8GfAylpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.title(\"Image label is: {}\".format(train_labels[i]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L2tmpZFkyQ_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_images.reshape(-1,28,28,1)\n",
        "x_test = test_images.reshape(-1,28,28,1)"
      ],
      "metadata": {
        "id": "RFbc84Y4yQ81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning and Building Keras Model"
      ],
      "metadata": {
        "id": "TvP0881sypMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For hypertuning the Keras model I am using Keras Tuner which is a TensorFlow library that helps you pick the optimal set of hyperparameters. Additional information and documentation over the Keras Tuner library [here](https://www.tensorflow.org/tutorials/keras/keras_tuner)."
      ],
      "metadata": {
        "id": "KeCds0GNys17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting the parameters for the model to\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential([\n",
        "\n",
        "    # First conv_block\n",
        "    keras.layers.Conv2D(\n",
        "        filters = hp.Choice('conv_1_filter', values=[16, 32, 64, 128]),\n",
        "        kernel_size=hp.Choice('conv_1_kernel', values = [3,4]),\n",
        "        activation='relu',\n",
        "        input_shape=(28,28,1)),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    # Second conv_block\n",
        "    keras.layers.Conv2D(\n",
        "        filters = hp.Choice('conv_2_filter', values=[16, 32, 64, 128]),\n",
        "        kernel_size=hp.Choice('conv_2_kernel', values = [3,4]),\n",
        "        activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    # --------------------------------\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units = hp.Choice('units', values=[16, 32, 64, 128, 256]),\n",
        "                       activation='relu'),\n",
        "    keras.layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5)),\n",
        "\n",
        "    # --------------------------------\n",
        "    keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',\n",
        "                                                            values=[1e-1, 1e-2, 1e-3, 1e-4])),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "9q3sdXxuyQ6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the [Hyperband](https://www.tensorflow.org/tutorials/keras/keras_tuner#:~:text=The%20Hyperband%20tuning%20algorithm%20uses%20adaptive%20resource%20allocation%20and%20early%2Dstopping%20to%20quickly%20converge%20on%20a%20high%2Dperforming%20model.) tuning algorithm to find the optimal parameters."
      ],
      "metadata": {
        "id": "BLkE9BZTywtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective=\"val_accuracy\",\n",
        "                     max_epochs=5,\n",
        "                     factor=3,\n",
        "                     hyperband_iterations=3)"
      ],
      "metadata": {
        "id": "9l64j5_6yQ4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##The combination of parameters that the Hyperband tuning algorithm will run through\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "id": "dFQurpenyQ1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[EarlyStopping](https://keras.io/api/callbacks/early_stopping/) in Keras allows us to stop training the model early if the model stops improving."
      ],
      "metadata": {
        "id": "VbLl7bV0y08D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "QKbeQlHvyQzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code runs through all possible model combinations as specified in the build_model function above.\n",
        "\n",
        "**Note:** Keras Tuner took over 2 hours for the following code block to run, but I also left out the \"early_stop\" arguement, which may have allowed it to end sooner."
      ],
      "metadata": {
        "id": "DUn6uuZny4OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train,train_labels, epochs=3, validation_split=0.2, callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "PvWgs1oqyQwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimal Hyperparameters generated from the Keras Search"
      ],
      "metadata": {
        "id": "K3LoIrc4y9Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_hps = best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"\"\"conv_1_filter is {best_hps.get('conv_1_filter')}\"\"\")\n",
        "print(f\"\"\"conv_1_kernel is {best_hps.get('conv_1_kernel')}\"\"\")\n",
        "print(f\"\"\"conv_2_filter is {best_hps.get('conv_2_filter')}\"\"\")\n",
        "print(f\"\"\"conv_2_kernel is {best_hps.get('conv_2_kernel')}\"\"\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print(f\"\"\"units is {best_hps.get('units')}\"\"\")\n",
        "print(f\"\"\"learning_rate is {best_hps.get('learning_rate')}\"\"\")\n",
        "print(f\"\"\"dropout is {best_hps.get('dropout')}\"\"\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print(f\"\"\"The hyperparameter search is complete. The optimal number of units in the first densely-connected layer\n",
        "is {best_hps.get('units')} and the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\"\"\")"
      ],
      "metadata": {
        "id": "WAIdAkwFyQuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Keras Model"
      ],
      "metadata": {
        "id": "IfoW-eBNzBKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tuner.hypermodel.build(best_hps)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ae57thgRyQr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Now that we have the optimal hyperparamters for the model and data, we can use these to train and build the model.\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(x_train, train_labels, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "id": "Se9tRte9yQpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Running the model with the optimal hyperparameters\n",
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "history = hypermodel.fit(x_train, train_labels,\n",
        "                         epochs=best_epoch,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "dI6TS7aKyQmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Structure and Summary"
      ],
      "metadata": {
        "id": "NeSW9W_GzHiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel.summary()"
      ],
      "metadata": {
        "id": "RgzGNHNvyQkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Plot of the layers from the optimal hypermodel\n",
        "keras.utils.plot_model(hypermodel, show_shapes=True)"
      ],
      "metadata": {
        "id": "iWtFYQ2UyQhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras Model Evaluation and Performance"
      ],
      "metadata": {
        "id": "_7dANevzzNFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = hypermodel.predict(x_test)\n",
        "\n",
        "print(\"Prediction is -> {}\".format(pred[12]))\n",
        "print(\"Actual value is -> {}\".format(test_labels[12]))\n",
        "print(\"The highest value for label is {}\".format(np.argmax(pred[12])))"
      ],
      "metadata": {
        "id": "1mbTFTYFyQfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sMRZbKOjyQc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Validation/Test data does not seem to have as good of accuracy as the Training dataset had. However, for the the sake of time I reduced the number of Epochs for the Test dataset from 50 to 10, which might have reduced the chances for the model the improve further."
      ],
      "metadata": {
        "id": "7s3j8twgzRJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_aQFWUGyK0D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training and validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras Model with Hyperparameter Tuning did produce fairly good results, with an Accuracy on the test images of 90.24%. However, using the Keras Search function to find the optimal parameters did take over 2 hours to find, which seems to be pretty computationally heavy for the accuracy received."
      ],
      "metadata": {
        "id": "0a27xKl0zVlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = hypermodel.evaluate(x_test, test_labels)\n",
        "print(\"test loss:\", f\"{eval_result[0]:.2%}\")\n",
        "print(\"test accuracy:\", f\"{eval_result[1]:.2%}\")"
      ],
      "metadata": {
        "id": "Gu3_S3MNzUIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onFpPvFBzUGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Model"
      ],
      "metadata": {
        "id": "AKsjy38_zfi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "58WNyNbQzcQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, re\n",
        "import pickle, gzip\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "import lightgbm as lgb\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\\\n",
        "\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import log_loss, precision_recall_curve, average_precision_score, roc_curve, auc, roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "import keras\n",
        "from keras import backend as K, regularizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, BatchNormalization, Input, Lambda\n",
        "from keras.layers import\n",
        "from keras.losses import mse, binary_crossentropy"
      ],
      "metadata": {
        "id": "BhG-91YxzUBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Autoencoder Model"
      ],
      "metadata": {
        "id": "tr_puQZczwNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading in the data, Splitting data into Training/Testing datasets, and reshapping the images"
      ],
      "metadata": {
        "id": "TkpYHD4wzyaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('fashion-mnist_train.csv')\n",
        "test = pd.read_csv('fashion-mnist_test.csv')"
      ],
      "metadata": {
        "id": "5aj9vqLYzT81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "dQd_fWZTzT6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "rpxIBB5jzT4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding in random noise to the image data"
      ],
      "metadata": {
        "id": "WC86AGlQz3ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)\n",
        "\n",
        "x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\n",
        "x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)"
      ],
      "metadata": {
        "id": "PHdwA0J7zT1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing some examples of the clothing data with the noise added"
      ],
      "metadata": {
        "id": "3LQGiosiz6zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.title(\"original + noise\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
        "    plt.gray()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WzlRJ849zTzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a class called \"Denoise\" that will take the images that have the random noise added, and try to predict what type of clothing that noise-added image is."
      ],
      "metadata": {
        "id": "sUOmDhaiz9qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Denoise(Model):\n",
        "  def __init__(self):\n",
        "    super(Denoise, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Input(shape=(28, 28, 1)),\n",
        "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = Denoise()"
      ],
      "metadata": {
        "id": "MfNnMDlczTxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
      ],
      "metadata": {
        "id": "MCNhjrzJzTuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "CdRyDSfazTsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Autoencoder Model"
      ],
      "metadata": {
        "id": "n3QfZmQo0Dh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                verbose=1,\n",
        "                validation_data=(x_test_noisy, x_test))\n",
        "score = autoencoder.evaluate(x_test_noisy, x_test, verbose=0)"
      ],
      "metadata": {
        "id": "BYkaR7hazTqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Structure and Summary"
      ],
      "metadata": {
        "id": "VDiA6dHN0Hm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.encoder.summary()"
      ],
      "metadata": {
        "id": "e7FY_1qlzTns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.decoder.summary()"
      ],
      "metadata": {
        "id": "gjZFbAHYzTlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ],
      "metadata": {
        "id": "JcdnF-tUzTij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the noise-added images with the reconstructed images\n",
        "The reconstructed images are generated images from the random noise-added images."
      ],
      "metadata": {
        "id": "OEHcwRvk0MVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "\n",
        "    # display original + noise\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.title(\"original + noise\")\n",
        "    plt.imshow(tf.squeeze(x_test_noisy[i]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    bx = plt.subplot(2, n, i + n + 1)\n",
        "    plt.title(\"reconstructed\")\n",
        "    plt.imshow(tf.squeeze(decoded_imgs[i]))\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j-PllOla0MlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras Model Evaluation and Performance"
      ],
      "metadata": {
        "id": "mdAuTMYo0PbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for each class\n",
        "predicted_probs = autoencoder.predict(x_test)\n",
        "\n",
        "# Get the class indices with the highest probabilities\n",
        "predicted_classes = np.argmax(predicted_probs, axis=1)\n",
        "\n",
        "# Get the indices of the true class labels for validation data\n",
        "y_true = np.argmax(x_test, axis=1)\n",
        "\n",
        "# Find indices of correct and incorrect predictions\n",
        "correct = np.nonzero(predicted_classes == y_true)[0]\n",
        "incorrect = np.nonzero(predicted_classes != y_true)[0]"
      ],
      "metadata": {
        "id": "1VPJoR480Pp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = len(correct) / (len(correct) + len(incorrect))\n",
        "print(\"Accuracy: {:.2%}\".format(accuracy))\n",
        "\n",
        "# Print loss\n",
        "loss = autoencoder.evaluate(x_test_noisy, x_test)\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "# Create a loss graph\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SPCho57C0Vjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test loss:', score)\n",
        "# print('Test accuracy:', max(history.history[\"val_loss\"]))"
      ],
      "metadata": {
        "id": "64zXdctW0UJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Autoencoder Test/Validation model performs just slightly worse than the training model, with the test loss of 0.665% and training loss of 0.660%.\n",
        "\n",
        "Training the Autoencoder Model was substantially faster than the time it took to train the Keras Model. However, the Keras Model did have a much higher accuracy, which I would take over the Autoencoder model."
      ],
      "metadata": {
        "id": "w-N6fQJM0X0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sources:\n",
        "* [TensorFlow Website](https://www.tensorflow.org/tutorials/generative/autoencoder)\n",
        "* [Kaggle Repo](https://www.kaggle.com/code/aksahaha/autoencoders)"
      ],
      "metadata": {
        "id": "JBA3SYO90abE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EDvwYlc0Z2g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}